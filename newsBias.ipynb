{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Bias analysis notebook\n",
    "#### Authors: Alexander Lambert, Casey Mathews, and Shivam Patel\n",
    "#### Githubs: alambe22, cmathew9, and spatel90\n",
    "## Description: \n",
    "The goal of this notebook is to analysze a given set of articles (found in datasets/articles.txt) with known biases and leanings to see if patterns can be determined in their writing. \n",
    "We analyze the following parts of the articles:\n",
    "- Buzzwords and phrases count\n",
    "- Emotional word count\n",
    "- Average word length\n",
    "- Use of words with negative connotations\n",
    "- Use of words with positive connotations\n",
    "- Use of words that indicate opinion (I think, I believe, etc.)\n",
    "- Use of words that indicate fact (We know, research indicates, etc)\n",
    "- First person pronoun usage (Does the author present this as their perspective, or as information)\n",
    "\n",
    "Using the data we gather, the hope is to find patterns that could be used to analyze new articles for bias or factuality.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Article class defintion and reading in\n",
    "class article:\n",
    "        __slots__ =[\"link\", \"bias\", \"cont\", \"buzz\", \"emo\", \"neg\", \"pos\", \"avgLen\", \n",
    "                    \"opi\", \"fac\", \"fPro\"]\n",
    "        def __init__ (self, link, bias):\n",
    "            self.link = link\n",
    "            self.bias = bias\n",
    "            \n",
    "articles = []\n",
    "with open(\"datasets/articles.txt\") as fin:\n",
    "    for line in fin:\n",
    "        lineList = line.rstrip().split(\" \")\n",
    "        articles.append(article(lineList[1], lineList[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re\n",
    "\n",
    "# Return a list of all words in <img> alt text\n",
    "def get_alt_text(soup):\n",
    "    img_elements = soup.find_all(\"img\")\n",
    "    alt_text_words = []\n",
    "    for img_element in img_elements:\n",
    "        if('alt' in img_element):\n",
    "            img_text = img_element['alt']\n",
    "            img_text = string_cleaner(img_text)\n",
    "            img_text = img_text.lower()\n",
    "            words = list(filter(None, img_text.split(\" \")))\n",
    "            alt_text_words += words\n",
    "    return alt_text_words\n",
    "\n",
    "# Clean up a string for splitting on space\n",
    "def string_cleaner(paragraph):\n",
    "    # Remove apostrophes from the word\n",
    "    paragraph = paragraph.replace(\"'\", \"\")\n",
    "    paragraph = paragraph.replace(\"â€™\", \"\")\n",
    "    # Replace non-alpha-numeric characters with a space\n",
    "    paragraph = re.sub('[^A-Za-z]+', ' ', paragraph)\n",
    "    return paragraph\n",
    "\n",
    "# Returns a list of all words in an article\n",
    "def get_article(url):\n",
    "    # Setup\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Contains a list of all words from <p> and <li> elements\n",
    "    article_words = []\n",
    "    \n",
    "    # Contains all <p> and <li> elements\n",
    "    p_li_elements = soup.find_all([\"p\", \"li\"])\n",
    "    for p_li_element in p_li_elements:\n",
    "        p_li = p_li_element.getText()\n",
    "        p_li = string_cleaner(p_li)\n",
    "        # Convert the string to lowercase\n",
    "        p_li = p_li.lower()\n",
    "        # Filter out empty strings created by cleaning\n",
    "        words = list(filter(None, p_li.split(\" \")))\n",
    "        article_words += words\n",
    "    \n",
    "    alt_text = get_alt_text(soup)\n",
    "    article_words += alt_text\n",
    "    return \" \".join(article_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePhraseFile(file, phrases):\n",
    "    with open(file) as fin:\n",
    "        for line in fin:\n",
    "            phrases.append(line.strip())\n",
    "\n",
    "def filterWords(file, words):\n",
    "    phrases = []\n",
    "    count = 0\n",
    "    parsePhraseFile(file, phrases)\n",
    "    cur_pat = None\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        cur_pat = re.compile(r'\\b'+re.escape(phrase)+r'\\b')\n",
    "        count += len(cur_pat.findall(words))\n",
    "    return count\n",
    "    \n",
    "def wordLength(words):\n",
    "    sum = 0\n",
    "    wordList = words.split(' ')\n",
    "    for word in wordList:\n",
    "        sum = sum + len(word)\n",
    "    \n",
    "    return sum/len(wordList)\n",
    "\n",
    "for article in articles:\n",
    "    print(\"Processing article:\" + article.link)\n",
    "    article.cont = get_article(article.link)\n",
    "    article.buzz = filterWords(\"./datasets/buzzwords.txt\", article.cont)\n",
    "    article.emo = filterWords(\"./datasets/emotional_words.txt\", article.cont)\n",
    "    article.neg = filterWords(\"./datasets/negative-words.txt\", article.cont)\n",
    "    article.pos = filterWords(\"./datasets/positive-words.txt\", article.cont)\n",
    "    article.avgLen = wordLength(article.cont)\n",
    "    article.opi = filterWords(\"./datasets/opinion.txt\", article.cont)\n",
    "    article.fac = filterWords(\"./datasets/fact_phrases.txt\", article.cont)\n",
    "    article.fPro = filterWords(\"./datasets/first_person.txt\", article.cont)\n",
    "    print(\"Processed article:\" + article.link +\" avg_len: \" + str(article.avgLen) + \" negative:\" + str(article.neg) + \" positive:\" + str(article.pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
